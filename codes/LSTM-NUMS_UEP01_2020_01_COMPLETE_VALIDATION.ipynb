{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7633,
     "status": "ok",
     "timestamp": 1685185295899,
     "user": {
      "displayName": "Bruno Sartori",
      "userId": "07295881313324234848"
     },
     "user_tz": 180
    },
    "id": "2aNJ8YmyW5BP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BIBLIOTECAS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from array import array\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7633,
     "status": "ok",
     "timestamp": 1685185295899,
     "user": {
      "displayName": "Bruno Sartori",
      "userId": "07295881313324234848"
     },
     "user_tz": 180
    },
    "id": "2aNJ8YmyW5BP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "def productors_adjustment(panda):\n",
    "    panda['DATE'] = pd.to_datetime(panda['DATE'], dayfirst=True)\n",
    "    panda['DATE'] = panda['DATE'].apply(lambda x:x.toordinal())\n",
    "\n",
    "    # replace null values with 0\n",
    "    for i in range(panda.shape[1]):\n",
    "        panda.iloc[:, i] = panda.iloc[:, i].fillna(0)\n",
    "    \n",
    "    #panda = panda[['QO', 'QG', 'QW']]\n",
    "        \n",
    "def forecast_adjustment(panda):\n",
    "    panda['DATE'] = pd.to_datetime(panda['DATE'], dayfirst=True, format='mixed')\n",
    "    panda['DATE'] = panda['DATE'].apply(lambda x:x.toordinal())\n",
    "    \n",
    "    # replace null values with 0\n",
    "    for i in range(panda.shape[1]):\n",
    "        panda.iloc[:, i] = panda.iloc[:, i].fillna(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7633,
     "status": "ok",
     "timestamp": 1685185295899,
     "user": {
      "displayName": "Bruno Sartori",
      "userId": "07295881313324234848"
     },
     "user_tz": 180
    },
    "id": "2aNJ8YmyW5BP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Productors Data from UEP 01\n",
    "p_uep01 = []\n",
    "for i in range(8):\n",
    "    file_path = \"/projetos/bekv/Mestrado/DADOS/HIST/hist_P\" + str(i+1) + \"_01.csv\"\n",
    "    p_uep01.append(pd.read_csv(file_path, delimiter=\";\", decimal=\",\"))\n",
    "    productors_adjustment(p_uep01[i])\n",
    "\n",
    "#Productors Data from UEP 02\n",
    "p_uep02 = []\n",
    "for i in range(9):\n",
    "    file_path = \"/projetos/bekv/Mestrado/DADOS/HIST/hist_P\" + str(i+1) + \"_02.csv\"\n",
    "    p_uep02.append(pd.read_csv(file_path, delimiter=\";\", decimal=\",\"))\n",
    "    productors_adjustment(p_uep02[i])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v_UEP = 1\n",
    "\n",
    "v_WINDOW = 60\n",
    "v_BATCH = 128\n",
    "v_EPOCHS =  25\n",
    "v_UNITS = 50\n",
    "v_DROPOUT = 0.0\n",
    "\n",
    "v_RANDOM = 11032019\n",
    "\n",
    "v_SCENARIO = \"LSTM-NUMS\"\n",
    "\n",
    "GRAPH_TIME = \"2020/01\"\n",
    "GRAPH_YEAR = \"2020\"\n",
    "GRAPH_MONTH = \"01\"\n",
    "\n",
    "if (GRAPH_TIME == \"2020/01\"):\n",
    "    SIMULATION_FILE = 0\n",
    "else:\n",
    "    if (GRAPH_TIME == \"2020/06\"):\n",
    "        SIMULATION_FILE = 1\n",
    "    else: \n",
    "        if (GRAPH_TIME == \"2021/01\"):\n",
    "            SIMULATION_FILE = 2\n",
    "        else:\n",
    "            if (GRAPH_TIME == \"2021/06\"):\n",
    "                SIMULATION_FILE = 3\n",
    "            else:\n",
    "                if (GRAPH_TIME == \"2022/01\"):\n",
    "                    SIMULATION_FILE = 4\n",
    "                else:\n",
    "                    if (GRAPH_TIME == \"2022/06\"):\n",
    "                        SIMULATION_FILE = 5\n",
    "                    else:\n",
    "                        SIMULATION_FILE = 6\n",
    "\n",
    "SELECTED_WELL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Forecasts for wells and Platforms\n",
    "forecast_sim = []\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_01_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_06_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_01_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_06_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_01_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_06_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_01_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0])) # not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(7):\n",
    "    forecast_adjustment(forecast_sim[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_start = min(p_uep01[0].DATE)\n",
    "date_start\n",
    "\n",
    "for i in range(8):\n",
    "    p_uep01[i]['DATE'] = p_uep01[i]['DATE'] - date_start\n",
    "    p_uep02[i]['DATE'] = p_uep02[i]['DATE'] - date_start\n",
    "\n",
    "p_uep02[8]['DATE'] = p_uep02[8]['DATE'] - date_start\n",
    "    \n",
    "for i in range(7):\n",
    "    forecast_sim[i]['DATE'] = forecast_sim[i]['DATE'] - date_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_hist = (p_uep01[0]['DATE'] < forecast_sim[SIMULATION_FILE]['DATE'][0])\n",
    "\n",
    "TOTAL_EXTRAPOLATION = 360\n",
    "\n",
    "filter_extr = ((p_uep01[0]['DATE'] >= forecast_sim[SIMULATION_FILE]['DATE'][0]) & (p_uep01[0]['DATE'] < forecast_sim[SIMULATION_FILE]['DATE'][0] + TOTAL_EXTRAPOLATION))\n",
    "\n",
    "p_01_training_set = []\n",
    "p_01_performed_set = []\n",
    "for i in range(8):\n",
    "    p_01_training_set.append(p_uep01[i][filter_hist])\n",
    "    p_01_training_set[i] = p_01_training_set[i].drop('DATE', axis=1)\n",
    "    p_01_performed_set.append(p_uep01[i][filter_extr])\n",
    "    p_01_performed_set[i] = p_01_performed_set[i].drop('DATE', axis=1)\n",
    "    \n",
    "p_02_training_set = []\n",
    "p_02_performed_set = []\n",
    "for i in range(9):\n",
    "    p_02_training_set.append(p_uep02[i][filter_hist])\n",
    "    p_02_training_set[i] = p_02_training_set[i].drop('DATE', axis=1)\n",
    "    p_02_performed_set.append(p_uep02[i][filter_extr])\n",
    "    p_02_performed_set[i] = p_02_performed_set[i].drop('DATE', axis=1)\n",
    "    \n",
    "for i in range(7):\n",
    "    forecast_sim[i] = forecast_sim[i].drop('DATE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_01_training_set_total = p_01_training_set[0][['QO', 'QG', 'QW']]\n",
    "p_01_performed_set_total = p_01_performed_set[0][['QO', 'QG', 'QW']]\n",
    "for SELECTED_WELL in range(1,8):\n",
    "    p_01_training_set_total += p_01_training_set[SELECTED_WELL][['QO', 'QG', 'QW']]\n",
    "    p_01_performed_set_total += p_01_performed_set[SELECTED_WELL][['QO', 'QG', 'QW']]\n",
    "\n",
    "p_02_training_set_total = p_02_training_set[0][['QO', 'QG', 'QW']]\n",
    "p_02_performed_set_total = p_02_performed_set[0][['QO', 'QG', 'QW']]\n",
    "for SELECTED_WELL in range(1,9):\n",
    "    p_02_training_set_total += p_02_training_set[SELECTED_WELL][['QO', 'QG', 'QW']]\n",
    "    p_02_performed_set_total += p_02_performed_set[SELECTED_WELL][['QO', 'QG', 'QW']]\n",
    "    \n",
    "forecast_sim_total = forecast_sim[0][['QO_01', 'QG_01', 'QW_01']]\n",
    "for SIM in range(1,7):\n",
    "    forecast_sim_total += forecast_sim[SIM][['QO_01', 'QG_01', 'QW_01']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_01_scaled_train_set_total = copy.deepcopy(p_01_training_set_total)\n",
    "p_02_scaled_train_set_total = copy.deepcopy(p_02_training_set_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Training Data Normalization\n",
    "\n",
    "#UEP 01\n",
    "for feat in range(p_01_scaled_train_set_total.shape[1]):\n",
    "    scaler.fit(p_01_training_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "    p_01_scaled_train_set_total[p_01_scaled_train_set_total.columns[feat]] = scaler.transform(p_01_scaled_train_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "\n",
    "#UEP 02\n",
    "for feat in range(p_02_scaled_train_set_total.shape[1]):\n",
    "    scaler.fit(p_02_training_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "    p_02_scaled_train_set_total[p_02_scaled_train_set_total.columns[feat]] = scaler.transform(p_02_scaled_train_set_total.iloc[:,feat].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (v_UEP == 1):\n",
    "    p_01_scaled_train_set_total = p_01_scaled_train_set_total.values\n",
    "else:\n",
    "    p_02_scaled_train_set_total = p_02_scaled_train_set_total.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_UEP_01(simulation_file):\n",
    "    \n",
    "    forecast = forecast_sim[simulation_file][:TOTAL_EXTRAPOLATION]\n",
    "    \n",
    "    QO = forecast.QO_01 / 0.93\n",
    "    QG = forecast.QG_01\n",
    "    QW = forecast.QW_01\n",
    "    \n",
    "    return QO, QG, QW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_UEP_02(simulation_file):\n",
    "    \n",
    "    forecast = forecast_sim[simulation_file][:TOTAL_EXTRAPOLATION]\n",
    "    \n",
    "    QO = forecast.QO_02 / 0.94\n",
    "    QG = forecast.QG_02\n",
    "    QW = forecast.QW_02\n",
    "    \n",
    "    return QO, QG, QW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def smape(actual, predicted) -> float:\n",
    "    value = 0    \n",
    "    for i in range(actual.shape[0]):\n",
    "        value = value + np.abs(actual[i][0] - predicted[i][0]) / ( (np.abs(actual[i][0]) + np.abs(predicted[i][0]) + 0.000001) )\n",
    "\n",
    "    return (round( 1 * value / actual.shape[0], 3))\n",
    "\n",
    "def print_smape(name, actual, predicted):\n",
    "    print(name + \" SymetricMeanAbsolutePercentageError (SMAPE):\" + str(smape(actual, predicted)))\n",
    "\n",
    "def file_smape(actual, predicted):\n",
    "    return str(smape(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def mape(actual, predicted) -> float:\n",
    "    value = 0    \n",
    "    for i in range(actual.shape[0]):\n",
    "        value = value + np.abs(actual[i][0] - predicted[i][0]) / ( (np.abs(actual[i][0]) ) + 0.000001)\n",
    "\n",
    "    return (round( 1 * value / actual.shape[0], 3))\n",
    "\n",
    "def print_mape(name, actual, predicted):\n",
    "    print(name + \" MeanAbsolutePercentageError (MAPE):\" + str(mape(actual, predicted)))\n",
    "\n",
    "def file_mape(actual, predicted):\n",
    "    return str(mape(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def mae(actual, predicted) -> float:\n",
    "    value = 0    \n",
    "    for i in range(actual.shape[0]):\n",
    "        value = value + np.abs(actual[i][0] - predicted[i][0])\n",
    "\n",
    "    return (round( 1 * value / actual.shape[0], 3))\n",
    "\n",
    "def print_mae(name, actual, predicted):\n",
    "    print(name + \" MeanAbsoluteError (MAE):\" + str(mae(actual, predicted)))\n",
    "\n",
    "def file_mae(actual, predicted):\n",
    "    return str(mae(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def mse(actual, predicted) -> float:\n",
    "    value = 0    \n",
    "    for i in range(actual.shape[0]):\n",
    "        value = value + np.abs(actual[i][0] - predicted[i][0]) * np.abs(actual[i][0] - predicted[i][0])\n",
    "\n",
    "    return (round( 1 * value / actual.shape[0], 3))\n",
    "\n",
    "def print_mse(name, actual, predicted):\n",
    "    print(name + \" MeanSquaredError (MSE):\" + str(mse(actual, predicted)))\n",
    "\n",
    "def file_mse(actual, predicted):\n",
    "    return str(mse(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Construção da LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import random as python_random\n",
    "\n",
    "for v_WINDOW in [30, 60, 120]:\n",
    "    for v_BATCH in [16, 32, 64, 128]:\n",
    "        for v_EPOCHS in [25, 50, 100]:\n",
    "            for v_UNITS in [50, 100]:\n",
    "                for v_DROPOUT in [0.00, 0.10, 0.30]:\n",
    "                    \n",
    "                    print(\"2020_01;\" + str(v_WINDOW) + \";4;\" + str(v_BATCH) + \";\" + str(v_EPOCHS) + \";\" + str(v_UNITS) + \";\" + str(v_DROPOUT) + \";TANH;\" + \"RMSProp\")\n",
    "\n",
    "                    start_time = time.time()\n",
    "                     \n",
    "                    TAMANHO_JANELA = v_WINDOW\n",
    "                    testing_inputs = []\n",
    "                    y_pred = []\n",
    "                    \n",
    "                    X_train = []\n",
    "                    y_train = []\n",
    "                    if (v_UEP == 1):\n",
    "                        for i in range(TAMANHO_JANELA, p_01_scaled_train_set_total.shape[0]):\n",
    "                            X_train.append(p_01_scaled_train_set_total[i-TAMANHO_JANELA:i, :])\n",
    "                            y_train.append(p_01_scaled_train_set_total[i, :])\n",
    "                    else: \n",
    "                        for i in range(TAMANHO_JANELA, p_02_scaled_train_set_total.shape[0]):\n",
    "                            X_train.append(p_02_scaled_train_set_total[i-TAMANHO_JANELA:i, :])\n",
    "                            y_train.append(p_02_scaled_train_set_total[i, :])\n",
    "                    \n",
    "                    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "                    X_train.shape, y_train.shape\n",
    "                    \n",
    "                    np.random.seed(v_RANDOM) \n",
    "                    python_random.seed(v_RANDOM)\n",
    "                    tf.random.set_seed(v_RANDOM)\n",
    "                    \n",
    "                    regressor = Sequential()\n",
    "                    regressor.add(LSTM(units=v_UNITS, activation = 'tanh', return_sequences = True))\n",
    "                    regressor.add(Dropout(v_DROPOUT))\n",
    "                    regressor.add(LSTM(units=v_UNITS, activation = 'tanh', return_sequences = True))\n",
    "                    regressor.add(Dropout(v_DROPOUT))\n",
    "                    regressor.add(LSTM(units=v_UNITS, activation = 'tanh', return_sequences = True))\n",
    "                    regressor.add(Dropout(v_DROPOUT))\n",
    "                    regressor.add(LSTM(units=v_UNITS, activation = 'tanh'))\n",
    "                    regressor.add(Dropout(v_DROPOUT))\n",
    "                    regressor.add(Dense(units=X_train.shape[2]))\n",
    "                    regressor.compile(optimizer='RMSProp', loss='mean_squared_error') # better choices previously tested\n",
    "                    \n",
    "                    history = regressor.fit(X_train, y_train, epochs=v_EPOCHS, batch_size=v_BATCH, shuffle=True, verbose=0)\n",
    "                        \n",
    "                    df = []\n",
    "                    if (v_UEP == 1):\n",
    "                        df = np.append(p_01_training_set_total[-TAMANHO_JANELA:], np.array(forecast_UEP_01(SIMULATION_FILE)).T, axis = 0)\n",
    "                        for feat in range(df.shape[1]):\n",
    "                            scaler.fit(p_01_training_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "                            df[:,feat:feat+1] = scaler.transform(df[:,feat].reshape(-1,1))\n",
    "                    else:\n",
    "                        df = np.append(p_02_training_set_total[-TAMANHO_JANELA:], np.array(forecast_UEP_02(SIMULATION_FILE)).T, axis = 0)    \n",
    "                        for feat in range(df.shape[1]):\n",
    "                            scaler.fit(p_02_training_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "                            df[:,feat:feat+1] = scaler.transform(df[:,feat].reshape(-1,1))\n",
    "                        \n",
    "                    testing_inputs = df\n",
    "                        \n",
    "                    X_test = []\n",
    "                    y_test = []\n",
    "                    for i in range(TAMANHO_JANELA, testing_inputs.shape[0]):\n",
    "                        X_test.append(testing_inputs[i-TAMANHO_JANELA:i])\n",
    "                        y_test.append(testing_inputs[i, 0])\n",
    "                    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "                    y_pred = regressor.predict(X_test)\n",
    "\n",
    "                    #Eliminate all negative values to oil, water and gas productions\n",
    "                    y_pred[:,0][y_pred[:,0] < 0] = 0\n",
    "                    y_pred[:,1][y_pred[:,1] < 0] = 0\n",
    "                    y_pred[:,2][y_pred[:,2] < 0] = 0\n",
    "\n",
    "                    scaler.fit(p_01_training_set_total.iloc[:,0].values.reshape(-1,1))\n",
    "                    oil_predicted_UEP01 = scaler.inverse_transform(y_pred[:,0].reshape(-1, 1))\n",
    "                    oil_predicted_UEP01[oil_predicted_UEP01 > 19080] = 19080 # SPU02 processing limit\n",
    "                    \n",
    "                    oil_simulated_UEP01 = scaler.inverse_transform(testing_inputs[:,0][-TOTAL_EXTRAPOLATION:].reshape(-1, 1))\n",
    "                    \n",
    "                    oil_performed_UEP01 = p_01_performed_set_total.QO.values \n",
    "                    \n",
    "                    scaler.fit(oil_performed_UEP01.reshape(-1,1))\n",
    "\n",
    "                    string_final = \"\"\n",
    "                    string_final = (\"2020_01;\" + str(v_WINDOW) + \";4;\" + str(v_BATCH) + \";\" + str(v_EPOCHS) + \";\" + \n",
    "                                    str(v_UNITS) + \";\" + str(v_DROPOUT) + \";TANH;\" + \"RMSProp;\" +                                  \n",
    "                                    file_smape(scaler.transform(oil_predicted_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                                    file_smape(scaler.transform(oil_simulated_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                                    file_mae(scaler.transform(oil_predicted_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                                    file_mae(scaler.transform(oil_simulated_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                                    file_mape(scaler.transform(oil_predicted_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                                    file_mape(scaler.transform(oil_simulated_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                                    file_mse(scaler.transform(oil_predicted_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                                    file_mse(scaler.transform(oil_simulated_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))))\n",
    "                    \n",
    "                    string_final = (string_final + \";\" + \n",
    "                                    str(round(np.mean(oil_performed_UEP01),2)) + \";\" +\n",
    "                                    str(round(np.mean(oil_predicted_UEP01),2)) + \";\" +\n",
    "                                    \"0;\" + # to be filled in later with the difference between the productions in the spreadsheet\n",
    "                                    str(round(np.mean(oil_simulated_UEP01),2)) + \";\" +\n",
    "                                    \"0\") # to be filled in later with the difference between the productions in the spreadsheet\n",
    "\n",
    "                    scaler.fit(p_01_training_set_total.iloc[:,2].values.reshape(-1,1))\n",
    "    \n",
    "                    wat_predicted_UEP01 = scaler.inverse_transform(y_pred[:,2].reshape(-1, 1))           \n",
    "                    wat_simulated_UEP01 = scaler.inverse_transform(testing_inputs[:,2][-TOTAL_EXTRAPOLATION:].reshape(-1, 1))          \n",
    "                    wat_performed_UEP01 = p_01_performed_set_total.QW.values\n",
    "                    \n",
    "                    scaler.fit(wat_performed_UEP01.reshape(-1,1))\n",
    "                    string_final = (string_final + \";\" +\n",
    "                                    file_smape(scaler.transform(wat_predicted_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                                    file_smape(scaler.transform(wat_simulated_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                                    file_mae(scaler.transform(wat_predicted_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                                    file_mae(scaler.transform(wat_simulated_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                                    file_mape(scaler.transform(wat_predicted_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                                    file_mape(scaler.transform(wat_simulated_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                                    file_mse(scaler.transform(wat_predicted_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                                    file_mse(scaler.transform(wat_simulated_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                                    str(round(np.mean(wat_performed_UEP01),2)) + \";\" +\n",
    "                                    str(round(np.mean(wat_predicted_UEP01),2)) + \";\" +\n",
    "                                    \"0;\" + \n",
    "                                    str(round(np.mean(wat_simulated_UEP01),2)) + \";\" +\n",
    "                                    \"0;\" + str((time.time() - start_time)/60))\n",
    "\n",
    "                    with open(\"_LSTM_NUMS_VALIDATION_RESULTS.txt\", \"a\") as file:\n",
    "                        file.write(string_final + \"\\n\") \n",
    "                        print(string_final)\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNN3ZwJQwCWFoijd7P1aFSB",
   "provenance": [
    {
     "file_id": "1DBXC3iVjUN5TgbRIwjYYb9D8uTfLJtFg",
     "timestamp": 1685057630275
    },
    {
     "file_id": "1O_Hs9RMKlVBQ_C33yeVLbMp36EtllVXz",
     "timestamp": 1685057555020
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
