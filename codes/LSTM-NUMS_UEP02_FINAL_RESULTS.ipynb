{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7633,
     "status": "ok",
     "timestamp": 1685185295899,
     "user": {
      "displayName": "Bruno Sartori",
      "userId": "07295881313324234848"
     },
     "user_tz": 180
    },
    "id": "2aNJ8YmyW5BP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BIBLIOTECAS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "from array import array\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "import csv\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7633,
     "status": "ok",
     "timestamp": 1685185295899,
     "user": {
      "displayName": "Bruno Sartori",
      "userId": "07295881313324234848"
     },
     "user_tz": 180
    },
    "id": "2aNJ8YmyW5BP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "def productors_adjustment(panda):\n",
    "    panda['DATE'] = pd.to_datetime(panda['DATE'], dayfirst=True)\n",
    "    panda['DATE'] = panda['DATE'].apply(lambda x:x.toordinal())\n",
    "\n",
    "    # replace null values with 0\n",
    "    for i in range(panda.shape[1]):\n",
    "        panda.iloc[:, i] = panda.iloc[:, i].fillna(0)\n",
    "    \n",
    "    #panda = panda[['QO', 'QG', 'QW']]\n",
    "        \n",
    "def forecast_adjustment(panda):\n",
    "    panda['DATE'] = pd.to_datetime(panda['DATE'], dayfirst=True, format='mixed')\n",
    "    panda['DATE'] = panda['DATE'].apply(lambda x:x.toordinal())\n",
    "    \n",
    "    # replace null values with 0\n",
    "    for i in range(panda.shape[1]):\n",
    "        panda.iloc[:, i] = panda.iloc[:, i].fillna(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7633,
     "status": "ok",
     "timestamp": 1685185295899,
     "user": {
      "displayName": "Bruno Sartori",
      "userId": "07295881313324234848"
     },
     "user_tz": 180
    },
    "id": "2aNJ8YmyW5BP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Productors Data from UEP 01\n",
    "p_uep01 = []\n",
    "for i in range(8):\n",
    "    file_path = \"/projetos/bekv/Mestrado/DADOS/HIST/hist_P\" + str(i+1) + \"_01.csv\"\n",
    "    p_uep01.append(pd.read_csv(file_path, delimiter=\";\", decimal=\",\"))\n",
    "    productors_adjustment(p_uep01[i])\n",
    "\n",
    "#Productors Data from UEP 02\n",
    "p_uep02 = []\n",
    "for i in range(9):\n",
    "    file_path = \"/projetos/bekv/Mestrado/DADOS/HIST/hist_P\" + str(i+1) + \"_02.csv\"\n",
    "    p_uep02.append(pd.read_csv(file_path, delimiter=\";\", decimal=\",\"))\n",
    "    productors_adjustment(p_uep02[i])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v_UEP = 2\n",
    "\n",
    "v_WINDOW = 60\n",
    "v_BATCH = 128 \n",
    "v_EPOCHS =  25 \n",
    "v_UNITS = 50 \n",
    "v_DROPOUT = 0.00\n",
    "\n",
    "v_RANDOM = 11032019\n",
    "\n",
    "v_SCENARIO = \"LSTM-NUMS\"\n",
    "\n",
    "if (GRAPH_TIME == \"2020/01\"):\n",
    "    SIMULATION_FILE = 0\n",
    "elif (GRAPH_TIME == \"2020/02\"):\n",
    "    SIMULATION_FILE = 1\n",
    "elif (GRAPH_TIME == \"2020/03\"):\n",
    "    SIMULATION_FILE = 2\n",
    "elif (GRAPH_TIME == \"2020/04\"):\n",
    "    SIMULATION_FILE = 3\n",
    "elif (GRAPH_TIME == \"2020/05\"):\n",
    "    SIMULATION_FILE = 4\n",
    "elif (GRAPH_TIME == \"2020/06\"):\n",
    "    SIMULATION_FILE = 5\n",
    "elif (GRAPH_TIME == \"2020/07\"):\n",
    "    SIMULATION_FILE = 6\n",
    "elif (GRAPH_TIME == \"2020/08\"):\n",
    "    SIMULATION_FILE = 7\n",
    "elif (GRAPH_TIME == \"2020/09\"):\n",
    "    SIMULATION_FILE = 8\n",
    "elif (GRAPH_TIME == \"2020/10\"):\n",
    "    SIMULATION_FILE = 9\n",
    "elif (GRAPH_TIME == \"2020/11\"):\n",
    "    SIMULATION_FILE = 10\n",
    "elif (GRAPH_TIME == \"2020/12\"):\n",
    "    SIMULATION_FILE = 11\n",
    "elif (GRAPH_TIME == \"2021/01\"):\n",
    "    SIMULATION_FILE = 12\n",
    "elif (GRAPH_TIME == \"2021/02\"):\n",
    "    SIMULATION_FILE = 13\n",
    "elif (GRAPH_TIME == \"2021/03\"):\n",
    "    SIMULATION_FILE = 14\n",
    "elif (GRAPH_TIME == \"2021/04\"):\n",
    "    SIMULATION_FILE = 15\n",
    "elif (GRAPH_TIME == \"2021/05\"):\n",
    "    SIMULATION_FILE = 16\n",
    "elif (GRAPH_TIME == \"2021/06\"):\n",
    "    SIMULATION_FILE = 17\n",
    "elif (GRAPH_TIME == \"2021/07\"):\n",
    "    SIMULATION_FILE = 18\n",
    "elif (GRAPH_TIME == \"2021/08\"):\n",
    "    SIMULATION_FILE = 19\n",
    "elif (GRAPH_TIME == \"2021/09\"):\n",
    "    SIMULATION_FILE = 20\n",
    "elif (GRAPH_TIME == \"2021/10\"):\n",
    "    SIMULATION_FILE = 21\n",
    "elif (GRAPH_TIME == \"2021/11\"):\n",
    "    SIMULATION_FILE = 22\n",
    "elif (GRAPH_TIME == \"2021/12\"):\n",
    "    SIMULATION_FILE = 23\n",
    "elif (GRAPH_TIME == \"2022/01\"):\n",
    "    SIMULATION_FILE = 24\n",
    "elif (GRAPH_TIME == \"2022/02\"):\n",
    "    SIMULATION_FILE = 25\n",
    "elif (GRAPH_TIME == \"2022/03\"):\n",
    "    SIMULATION_FILE = 26\n",
    "elif (GRAPH_TIME == \"2022/04\"):\n",
    "    SIMULATION_FILE = 27\n",
    "elif (GRAPH_TIME == \"2022/05\"):\n",
    "    SIMULATION_FILE = 28\n",
    "else:\n",
    "    SIMULATION_FILE = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Forecasts for wells and Platforms\n",
    "forecast_sim = []\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_01_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_02_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_03_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_04_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_05_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_06_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_07_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_08_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_09_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_10_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_11_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2020_12_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_01_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_02_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_03_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_04_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_05_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_06_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_07_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_08_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_09_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_10_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_11_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2021_12_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_01_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_02_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_03_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_04_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_05_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n",
    "forecast_sim.append(pd.read_csv(\"/projetos/bekv/Mestrado/DADOS/FORECAST/FORECAST_SIM_2022_06_daily.csv\", delimiter=\";\", decimal=\",\", skiprows=[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(forecast_sim)):\n",
    "    forecast_adjustment(forecast_sim[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_start = min(p_uep01[0].DATE)\n",
    "date_start\n",
    "\n",
    "for i in range(8):\n",
    "    p_uep01[i]['DATE'] = p_uep01[i]['DATE'] - date_start\n",
    "    p_uep02[i]['DATE'] = p_uep02[i]['DATE'] - date_start\n",
    "\n",
    "p_uep02[8]['DATE'] = p_uep02[8]['DATE'] - date_start\n",
    "    \n",
    "for i in range(len(forecast_sim)):\n",
    "    forecast_sim[i]['DATE'] = forecast_sim[i]['DATE'] - date_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter_hist = (p_uep01[0]['DATE'] < forecast_sim[SIMULATION_FILE]['DATE'][0])\n",
    "\n",
    "TOTAL_EXTRAPOLATION = 360\n",
    "\n",
    "filter_extr = ((p_uep01[0]['DATE'] >= forecast_sim[SIMULATION_FILE]['DATE'][0]) & (p_uep01[0]['DATE'] < forecast_sim[SIMULATION_FILE]['DATE'][0] + TOTAL_EXTRAPOLATION))\n",
    "\n",
    "p_01_training_set = []\n",
    "p_01_performed_set = []\n",
    "for i in range(8):\n",
    "    p_01_training_set.append(p_uep01[i][filter_hist])\n",
    "    p_01_training_set[i] = p_01_training_set[i].drop('DATE', axis=1)\n",
    "    p_01_performed_set.append(p_uep01[i][filter_extr])\n",
    "    p_01_performed_set[i] = p_01_performed_set[i].drop('DATE', axis=1)\n",
    "    \n",
    "p_02_training_set = []\n",
    "p_02_performed_set = []\n",
    "for i in range(9):\n",
    "    p_02_training_set.append(p_uep02[i][filter_hist])\n",
    "    p_02_training_set[i] = p_02_training_set[i].drop('DATE', axis=1)\n",
    "    p_02_performed_set.append(p_uep02[i][filter_extr])\n",
    "    p_02_performed_set[i] = p_02_performed_set[i].drop('DATE', axis=1)\n",
    "    \n",
    "for i in range(len(forecast_sim)):\n",
    "    forecast_sim[i] = forecast_sim[i].drop('DATE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_01_training_set_total = p_01_training_set[0][['QO', 'QG', 'QW']]\n",
    "p_01_performed_set_total = p_01_performed_set[0][['QO', 'QG', 'QW']]\n",
    "for SELECTED_WELL in range(1,8):\n",
    "    p_01_training_set_total += p_01_training_set[SELECTED_WELL][['QO', 'QG', 'QW']]\n",
    "    p_01_performed_set_total += p_01_performed_set[SELECTED_WELL][['QO', 'QG', 'QW']]\n",
    "\n",
    "p_02_training_set_total = p_02_training_set[0][['QO', 'QG', 'QW']]\n",
    "p_02_performed_set_total = p_02_performed_set[0][['QO', 'QG', 'QW']]\n",
    "for SELECTED_WELL in range(1,9):\n",
    "    p_02_training_set_total += p_02_training_set[SELECTED_WELL][['QO', 'QG', 'QW']]\n",
    "    p_02_performed_set_total += p_02_performed_set[SELECTED_WELL][['QO', 'QG', 'QW']]\n",
    "    \n",
    "forecast_sim_total = forecast_sim[0][['QO_01', 'QG_01', 'QW_01']]\n",
    "for SIM in range(1,len(forecast_sim)):\n",
    "    forecast_sim_total += forecast_sim[SIM][['QO_01', 'QG_01', 'QW_01']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_01_scaled_train_set_total = copy.deepcopy(p_01_training_set_total)\n",
    "p_02_scaled_train_set_total = copy.deepcopy(p_02_training_set_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Training Data Normalization\n",
    "\n",
    "#UEP 01\n",
    "for feat in range(p_01_scaled_train_set_total.shape[1]):\n",
    "    scaler.fit(p_01_training_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "    p_01_scaled_train_set_total[p_01_scaled_train_set_total.columns[feat]] = scaler.transform(p_01_scaled_train_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "\n",
    "#UEP 02\n",
    "for feat in range(p_02_scaled_train_set_total.shape[1]):\n",
    "    scaler.fit(p_02_training_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "    p_02_scaled_train_set_total[p_02_scaled_train_set_total.columns[feat]] = scaler.transform(p_02_scaled_train_set_total.iloc[:,feat].values.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (v_UEP == 1):\n",
    "    p_01_scaled_train_set_total = p_01_scaled_train_set_total.values\n",
    "else:\n",
    "    p_02_scaled_train_set_total = p_02_scaled_train_set_total.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_UEP_01(simulation_file):\n",
    "    \n",
    "    forecast = forecast_sim[simulation_file][:TOTAL_EXTRAPOLATION]\n",
    "    \n",
    "    QO = forecast.QO_01 / 0.93 #eficiency applied to numerical simulation\n",
    "    QG = forecast.QG_01\n",
    "    QW = forecast.QW_01\n",
    "    \n",
    "    return QO, QG, QW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_UEP_02(simulation_file):\n",
    "    \n",
    "    forecast = forecast_sim[simulation_file][:TOTAL_EXTRAPOLATION]\n",
    "    \n",
    "    QO = forecast.QO_02 / 0.94 #eficiency applied to numerical simulation\n",
    "    QG = forecast.QG_02\n",
    "    QW = forecast.QW_02\n",
    "    \n",
    "    return QO, QG, QW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def smape(actual, predicted) -> float:\n",
    "    value = 0    \n",
    "    for i in range(actual.shape[0]):\n",
    "        value = value + np.abs(actual[i][0] - predicted[i][0]) / ( (np.abs(actual[i][0]) + np.abs(predicted[i][0]) + 0.000001) )\n",
    "\n",
    "    return (round( 1 * value / actual.shape[0], 3))\n",
    "\n",
    "def print_smape(name, actual, predicted):\n",
    "    print(name + \" SymetricMeanAbsolutePercentageError (SMAPE):\" + str(smape(actual, predicted)))\n",
    "\n",
    "def file_smape(actual, predicted):\n",
    "    return str(smape(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def mape(actual, predicted) -> float:\n",
    "    value = 0    \n",
    "    for i in range(actual.shape[0]):\n",
    "        value = value + np.abs(actual[i][0] - predicted[i][0]) / ( (np.abs(actual[i][0]) ) + 0.000001)\n",
    "\n",
    "    return (round( 1 * value / actual.shape[0], 3))\n",
    "\n",
    "def print_mape(name, actual, predicted):\n",
    "    print(name + \" MeanAbsolutePercentageError (MAPE):\" + str(mape(actual, predicted)))\n",
    "\n",
    "def file_mape(actual, predicted):\n",
    "    return str(mape(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def mae(actual, predicted) -> float:\n",
    "    value = 0    \n",
    "    for i in range(actual.shape[0]):\n",
    "        value = value + np.abs(actual[i][0] - predicted[i][0])\n",
    "\n",
    "    return (round( 1 * value / actual.shape[0], 3))\n",
    "\n",
    "def print_mae(name, actual, predicted):\n",
    "    print(name + \" MeanAbsoluteError (MAE):\" + str(mae(actual, predicted)))\n",
    "\n",
    "def file_mae(actual, predicted):\n",
    "    return str(mae(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def mse(actual, predicted) -> float:\n",
    "    value = 0    \n",
    "    for i in range(actual.shape[0]):\n",
    "        value = value + np.abs(actual[i][0] - predicted[i][0]) * np.abs(actual[i][0] - predicted[i][0])\n",
    "\n",
    "    return (round( 1 * value / actual.shape[0], 3))\n",
    "\n",
    "def print_mse(name, actual, predicted):\n",
    "    print(name + \" MeanSquaredError (MSE):\" + str(mse(actual, predicted)))\n",
    "\n",
    "def file_mse(actual, predicted):\n",
    "    return str(mse(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Construção da LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import random as python_random\n",
    "\n",
    "if (v_UEP == 1):\n",
    "    oil_predicted_UEP01 = []\n",
    "    wat_predicted_UEP01 = []\n",
    "else:\n",
    "    oil_predicted_UEP02 = []\n",
    "    wat_predicted_UEP02 = []\n",
    "\n",
    "# Five evaluations with different random seeds\n",
    "for seed in range(5):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    TAMANHO_JANELA = v_WINDOW\n",
    "    testing_inputs = []\n",
    "    y_pred = []\n",
    "    \n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    if (v_UEP == 1):\n",
    "        for i in range(TAMANHO_JANELA, p_01_scaled_train_set_total.shape[0]):\n",
    "            X_train.append(p_01_scaled_train_set_total[i-TAMANHO_JANELA:i, :])\n",
    "            y_train.append(p_01_scaled_train_set_total[i, :])\n",
    "    else: \n",
    "        for i in range(TAMANHO_JANELA, p_02_scaled_train_set_total.shape[0]):\n",
    "            X_train.append(p_02_scaled_train_set_total[i-TAMANHO_JANELA:i, :])\n",
    "            y_train.append(p_02_scaled_train_set_total[i, :])\n",
    "    \n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_train.shape, y_train.shape\n",
    "    \n",
    "    np.random.seed(v_RANDOM) \n",
    "    python_random.seed(v_RANDOM)\n",
    "    tf.random.set_seed(v_RANDOM)\n",
    "    \n",
    "    regressor = Sequential()\n",
    "    regressor.add(LSTM(units=v_UNITS, activation = 'tanh', return_sequences = True))\n",
    "    regressor.add(Dropout(v_DROPOUT))\n",
    "    regressor.add(LSTM(units=v_WINDOW, activation = 'tanh', return_sequences = True))\n",
    "    regressor.add(Dropout(v_DROPOUT))\n",
    "    regressor.add(LSTM(units=v_WINDOW, activation = 'tanh', return_sequences = True))\n",
    "    regressor.add(Dropout(v_DROPOUT))\n",
    "    regressor.add(LSTM(units=v_WINDOW, activation = 'tanh'))\n",
    "    regressor.add(Dropout(v_DROPOUT))\n",
    "    regressor.add(Dense(units=X_train.shape[2]))\n",
    "    regressor.compile(optimizer='RMSProp', loss='mean_squared_error')\n",
    "    \n",
    "    history = regressor.fit(X_train, y_train, epochs=v_EPOCHS, batch_size=v_BATCH, shuffle=True, verbose=0)\n",
    "    \n",
    "    df = []\n",
    "    if (v_UEP == 1):\n",
    "        df = np.append(p_01_training_set_total[-TAMANHO_JANELA:], np.array(forecast_UEP_01(SIMULATION_FILE)).T, axis = 0)\n",
    "        for feat in range(df.shape[1]):\n",
    "            scaler.fit(p_01_training_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "            df[:,feat:feat+1] = scaler.transform(df[:,feat].reshape(-1,1))\n",
    "    else:\n",
    "        df = np.append(p_02_training_set_total[-TAMANHO_JANELA:], np.array(forecast_UEP_02(SIMULATION_FILE)).T, axis = 0)    \n",
    "        for feat in range(df.shape[1]):\n",
    "            scaler.fit(p_02_training_set_total.iloc[:,feat].values.reshape(-1,1))\n",
    "            df[:,feat:feat+1] = scaler.transform(df[:,feat].reshape(-1,1))\n",
    "\n",
    "    testing_inputs = df\n",
    "    \n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for i in range(TAMANHO_JANELA, testing_inputs.shape[0]):\n",
    "        X_test.append(testing_inputs[i-TAMANHO_JANELA:i])\n",
    "        y_test.append(testing_inputs[i, 0])\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "\n",
    "    #Eliminate all negative values to oil, water and gas productions\n",
    "    y_pred[:,0][y_pred[:,0] < 0] = 0\n",
    "    y_pred[:,1][y_pred[:,1] < 0] = 0\n",
    "    y_pred[:,2][y_pred[:,2] < 0] = 0\n",
    "\n",
    "    if (v_UEP == 1):\n",
    "        scaler.fit(p_01_training_set_total.iloc[:,0].values.reshape(-1,1))\n",
    "        oil_predicted_UEP01.append(scaler.inverse_transform(y_pred[:,0].reshape(-1, 1)))\n",
    "        oil_predicted_UEP01[seed][oil_predicted_UEP01[seed] > 19080] = 19080 # SPU01 processing limit\n",
    "                    \n",
    "        oil_simulated_UEP01 = scaler.inverse_transform(testing_inputs[:,0][-TOTAL_EXTRAPOLATION:].reshape(-1, 1))\n",
    "                    \n",
    "        oil_performed_UEP01 = p_01_performed_set_total.QO.values \n",
    "                    \n",
    "        scaler.fit(oil_performed_UEP01.reshape(-1,1))\n",
    "\n",
    "        string_final = \"\"\n",
    "        string_final = (GRAPH_TIME + \";\" + str(v_WINDOW) + \";4;\" + str(v_BATCH) + \";\" + str(v_EPOCHS) + \";\" + \n",
    "                        str(v_UNITS) + \";\" + str(v_DROPOUT) + \";TANH;\" + \"RMSProp;0;\" +                                  \n",
    "                        file_smape(scaler.transform(oil_predicted_UEP01[seed].reshape(-1,1))+1, scaler.transform(oil_performed_UEP01.reshape(-1,1))+1) + \";\" + \n",
    "                        file_smape(scaler.transform(oil_simulated_UEP01.reshape(-1,1))+1, scaler.transform(oil_performed_UEP01.reshape(-1,1))+1) + \";\" + \n",
    "                        file_mae(scaler.transform(oil_predicted_UEP01[seed].reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                        file_mae(scaler.transform(oil_simulated_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                        file_mape(scaler.transform(oil_predicted_UEP01[seed].reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                        file_mape(scaler.transform(oil_simulated_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                        file_mse(scaler.transform(oil_predicted_UEP01[seed].reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))) + \";\" + \n",
    "                        file_mse(scaler.transform(oil_simulated_UEP01.reshape(-1,1)), scaler.transform(oil_performed_UEP01.reshape(-1,1))))\n",
    "                    \n",
    "        string_final = (string_final + \";\" + \n",
    "                        str(round(np.mean(oil_performed_UEP01),2)) + \";\" +\n",
    "                        str(round(np.mean(oil_predicted_UEP01[seed]),2)) + \";\" +\n",
    "                        \"0;\" + # to be filled in later with the difference between the productions in the spreadsheet\n",
    "                        str(round(np.mean(oil_simulated_UEP01),2)) + \";\" +\n",
    "                        \"0\") # to be filled in later with the difference between the productions in the spreadsheet\n",
    "\n",
    "        scaler.fit(p_01_training_set_total.iloc[:,2].values.reshape(-1,1))\n",
    "    \n",
    "        wat_predicted_UEP01.append(scaler.inverse_transform(y_pred[:,2].reshape(-1, 1)))         \n",
    "        wat_simulated_UEP01 = scaler.inverse_transform(testing_inputs[:,2][-TOTAL_EXTRAPOLATION:].reshape(-1, 1))          \n",
    "        wat_performed_UEP01 = p_01_performed_set_total.QW.values\n",
    "                    \n",
    "        scaler.fit(wat_performed_UEP01.reshape(-1,1))\n",
    "        \n",
    "        string_final = (string_final + \";\" +\n",
    "                        file_smape(scaler.transform(wat_predicted_UEP01[seed].reshape(-1,1))+1, scaler.transform(wat_performed_UEP01.reshape(-1,1))+1) + \";\" +\n",
    "                        file_smape(scaler.transform(wat_simulated_UEP01.reshape(-1,1))+1, scaler.transform(wat_performed_UEP01.reshape(-1,1))+1) + \";\" +\n",
    "                        file_mae(scaler.transform(wat_predicted_UEP01[seed].reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                        file_mae(scaler.transform(wat_simulated_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                        file_mape(scaler.transform(wat_predicted_UEP01[seed].reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                        file_mape(scaler.transform(wat_simulated_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                        file_mse(scaler.transform(wat_predicted_UEP01[seed].reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                        file_mse(scaler.transform(wat_simulated_UEP01.reshape(-1,1)), scaler.transform(wat_performed_UEP01.reshape(-1,1))) + \";\" +\n",
    "                        str(round(np.mean(wat_performed_UEP01),2)) + \";\" +\n",
    "                        str(round(np.mean(wat_predicted_UEP01[seed]),2)) + \";\" +\n",
    "                        \"0;\" + \n",
    "                        str(round(np.mean(wat_simulated_UEP01),2)) + \";\" +\n",
    "                        \"0;\" + str((time.time() - start_time)/60))\n",
    "    else:\n",
    "        scaler.fit(p_02_training_set_total.iloc[:,0].values.reshape(-1,1))\n",
    "        oil_predicted_UEP02.append(scaler.inverse_transform(y_pred[:,0].reshape(-1, 1)))\n",
    "        oil_predicted_UEP02[seed][oil_predicted_UEP02[seed] > 23850] = 23850 # SPU02 processing limit\n",
    "                    \n",
    "        oil_simulated_UEP02 = scaler.inverse_transform(testing_inputs[:,0][-TOTAL_EXTRAPOLATION:].reshape(-1, 1))\n",
    "                    \n",
    "        oil_performed_UEP02 = p_02_performed_set_total.QO.values \n",
    "                    \n",
    "        scaler.fit(oil_performed_UEP02.reshape(-1,1))\n",
    "\n",
    "        string_final = \"\"\n",
    "        string_final = (GRAPH_TIME + \";\" + str(v_WINDOW) + \";4;\" + str(v_BATCH) + \";\" + str(v_EPOCHS) + \";\" + \n",
    "                        str(v_UNITS) + \";\" + str(v_DROPOUT) + \";TANH;\" + \"RMSProp;0;\" +                                  \n",
    "                        file_smape(scaler.transform(oil_predicted_UEP02[seed].reshape(-1,1))+1, scaler.transform(oil_performed_UEP02.reshape(-1,1))+1) + \";\" + \n",
    "                        file_smape(scaler.transform(oil_simulated_UEP02.reshape(-1,1))+1, scaler.transform(oil_performed_UEP02.reshape(-1,1))+1) + \";\" + \n",
    "                        file_mae(scaler.transform(oil_predicted_UEP02[seed].reshape(-1,1)), scaler.transform(oil_performed_UEP02.reshape(-1,1))) + \";\" + \n",
    "                        file_mae(scaler.transform(oil_simulated_UEP02.reshape(-1,1)), scaler.transform(oil_performed_UEP02.reshape(-1,1))) + \";\" + \n",
    "                        file_mape(scaler.transform(oil_predicted_UEP02[seed].reshape(-1,1)), scaler.transform(oil_performed_UEP02.reshape(-1,1))) + \";\" + \n",
    "                        file_mape(scaler.transform(oil_simulated_UEP02.reshape(-1,1)), scaler.transform(oil_performed_UEP02.reshape(-1,1))) + \";\" + \n",
    "                        file_mse(scaler.transform(oil_predicted_UEP02[seed].reshape(-1,1)), scaler.transform(oil_performed_UEP02.reshape(-1,1))) + \";\" + \n",
    "                        file_mse(scaler.transform(oil_simulated_UEP02.reshape(-1,1)), scaler.transform(oil_performed_UEP02.reshape(-1,1))))\n",
    "                    \n",
    "        string_final = (string_final + \";\" + \n",
    "                        str(round(np.mean(oil_performed_UEP02),2)) + \";\" +\n",
    "                        str(round(np.mean(oil_predicted_UEP02[seed]),2)) + \";\" +\n",
    "                        \"0;\" + # to be filled in later with the difference between the productions in the spreadsheet\n",
    "                        str(round(np.mean(oil_simulated_UEP02),2)) + \";\" +\n",
    "                        \"0\") # to be filled in later with the difference between the productions in the spreadsheet\n",
    "\n",
    "        scaler.fit(p_02_training_set_total.iloc[:,2].values.reshape(-1,1))\n",
    "    \n",
    "        wat_predicted_UEP02.append(scaler.inverse_transform(y_pred[:,2].reshape(-1, 1)))         \n",
    "        wat_simulated_UEP02 = scaler.inverse_transform(testing_inputs[:,2][-TOTAL_EXTRAPOLATION:].reshape(-1, 1))          \n",
    "        wat_performed_UEP02 = p_02_performed_set_total.QW.values\n",
    "                    \n",
    "        scaler.fit(wat_performed_UEP02.reshape(-1,1))\n",
    "        string_final = (string_final + \";\" +\n",
    "                        file_smape(scaler.transform(wat_predicted_UEP02[seed].reshape(-1,1))+1, scaler.transform(wat_performed_UEP02.reshape(-1,1))+1) + \";\" +\n",
    "                        file_smape(scaler.transform(wat_simulated_UEP02.reshape(-1,1))+1, scaler.transform(wat_performed_UEP02.reshape(-1,1))+1) + \";\" +\n",
    "                        file_mae(scaler.transform(wat_predicted_UEP02[seed].reshape(-1,1)), scaler.transform(wat_performed_UEP02.reshape(-1,1))) + \";\" +\n",
    "                        file_mae(scaler.transform(wat_simulated_UEP02.reshape(-1,1)), scaler.transform(wat_performed_UEP02.reshape(-1,1))) + \";\" +\n",
    "                        file_mape(scaler.transform(wat_predicted_UEP02[seed].reshape(-1,1)), scaler.transform(wat_performed_UEP02.reshape(-1,1))) + \";\" +\n",
    "                        file_mape(scaler.transform(wat_simulated_UEP02.reshape(-1,1)), scaler.transform(wat_performed_UEP02.reshape(-1,1))) + \";\" +\n",
    "                        file_mse(scaler.transform(wat_predicted_UEP02[seed].reshape(-1,1)), scaler.transform(wat_performed_UEP02.reshape(-1,1))) + \";\" +\n",
    "                        file_mse(scaler.transform(wat_simulated_UEP02.reshape(-1,1)), scaler.transform(wat_performed_UEP02.reshape(-1,1))) + \";\" +\n",
    "                        str(round(np.mean(wat_performed_UEP02),2)) + \";\" +\n",
    "                        str(round(np.mean(wat_predicted_UEP02),2)) + \";\" +\n",
    "                        \"0;\" + \n",
    "                        str(round(np.mean(wat_simulated_UEP02),2)) + \";\" +\n",
    "                        \"0;\" + str((time.time() - start_time)/60))        \n",
    "\n",
    "    with open(\"_LSTM_NUMS_FINAL_RESULTS_UEP0\" + str(v_UEP) + \".txt\", \"a\") as file:\n",
    "        file.write(string_final + \"\\n\") \n",
    "        print(string_final)\n",
    "      \n",
    "    v_RANDOM = v_RANDOM + 10 # generate a new random seed at each step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (v_UEP == 1):\n",
    "    # Produção de Óleo\n",
    "    scaler.fit(p_01_training_set_total.iloc[:,0].values.reshape(-1,1))\n",
    "    oil_simulated_UEP01 = scaler.inverse_transform(testing_inputs[:,0][-TOTAL_EXTRAPOLATION:].reshape(-1, 1)) \n",
    "    oil_performed_UEP01 = p_01_performed_set_total.QO.values \n",
    "    scaler.fit(oil_performed_UEP01.reshape(-1,1))\n",
    "    \n",
    "    oil_actual = scaler.transform(oil_performed_UEP01.reshape(-1,1))\n",
    "    oil_simulated = scaler.transform(oil_simulated_UEP01.reshape(-1,1))\n",
    "    oil_predicted = [scaler.transform(pred.reshape(-1,1)) for pred in oil_predicted_UEP01]\n",
    "    \n",
    "    # Produção de Água\n",
    "    scaler.fit(p_01_training_set_total.iloc[:,2].values.reshape(-1,1))\n",
    "    wat_simulated_UEP01 = scaler.inverse_transform(testing_inputs[:,2][-TOTAL_EXTRAPOLATION:].reshape(-1, 1))\n",
    "    wat_performed_UEP01 = p_01_performed_set_total.QW.values\n",
    "    scaler.fit(wat_performed_UEP01.reshape(-1,1))\n",
    "    \n",
    "    wat_actual = scaler.transform(wat_performed_UEP01.reshape(-1,1))\n",
    "    wat_simulated = scaler.transform(wat_simulated_UEP01.reshape(-1,1))\n",
    "    wat_predicted = [scaler.transform(pred.reshape(-1,1)) for pred in wat_predicted_UEP01]\n",
    "\n",
    "    medias = [np.mean(serie) for serie in oil_predicted_UEP01]\n",
    "    mediana_das_medias = np.median(medias)\n",
    "    indice_mediana = next(i for i, m in enumerate(medias) if np.isclose(m, mediana_das_medias))\n",
    "else:\n",
    "    # Produção de Óleo\n",
    "    scaler.fit(p_02_training_set_total.iloc[:,0].values.reshape(-1,1))\n",
    "    oil_simulated_UEP02 = scaler.inverse_transform(testing_inputs[:,0][-TOTAL_EXTRAPOLATION:].reshape(-1, 1)) \n",
    "    oil_performed_UEP02 = p_02_performed_set_total.QO.values \n",
    "    scaler.fit(oil_performed_UEP02.reshape(-1,1))\n",
    "    \n",
    "    oil_actual = scaler.transform(oil_performed_UEP02.reshape(-1,1))\n",
    "    oil_simulated = scaler.transform(oil_simulated_UEP02.reshape(-1,1))\n",
    "    oil_predicted = [scaler.transform(pred.reshape(-1,1)) for pred in oil_predicted_UEP02]\n",
    "    \n",
    "    # Produção de Água\n",
    "    scaler.fit(p_02_training_set_total.iloc[:,2].values.reshape(-1,1))\n",
    "    wat_simulated_UEP02 = scaler.inverse_transform(testing_inputs[:,2][-TOTAL_EXTRAPOLATION:].reshape(-1, 1))\n",
    "    wat_performed_UEP02 = p_02_performed_set_total.QW.values\n",
    "    scaler.fit(wat_performed_UEP02.reshape(-1,1))\n",
    "    \n",
    "    wat_actual = scaler.transform(wat_performed_UEP02.reshape(-1,1))\n",
    "    wat_simulated = scaler.transform(wat_simulated_UEP02.reshape(-1,1))\n",
    "    wat_predicted = [scaler.transform(pred.reshape(-1,1)) for pred in wat_predicted_UEP02]\n",
    "\n",
    "    medias = [np.mean(serie) for serie in oil_predicted_UEP02]\n",
    "    mediana_das_medias = np.median(medias)\n",
    "    indice_mediana = next(i for i, m in enumerate(medias) if np.isclose(m, mediana_das_medias))\n",
    "\n",
    "##\n",
    "# Criar figura com dois subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(9, 9))\n",
    "    \n",
    "# Gráfico de Óleo\n",
    "axs[0].plot(oil_actual, color='black', label='ACTUAL')\n",
    "axs[0].plot(oil_simulated, color='red', label='NUM SIMULATION')\n",
    "colors = ['#0CBCEB', '#0CBCEB', '#0CBCEB', '#0CBCEB', '#0CBCEB']\n",
    "          \n",
    "for i, pred in enumerate(oil_predicted):\n",
    "    linestyle = 'solid' if i == indice_mediana else 'dashed'\n",
    "    alpha = 1.0 if i == indice_mediana else 0.5\n",
    "    label = f'LSTM-NUMS/SEED{i+1}' + ('(S)' if i == indice_mediana else '')\n",
    "    axs[0].plot(pred, color=colors[i], linestyle=linestyle, alpha=alpha, label=label)\n",
    "axs[0].set_title(f'Oil Production/Prediction - SPU0{v_UEP}/F01 / from: {GRAPH_TIME}')\n",
    "axs[0].set_xlabel('Days')\n",
    "axs[0].set_ylabel('Oil Production')\n",
    "axs[0].legend()\n",
    "    \n",
    "# Gráfico de Água\n",
    "axs[1].plot(wat_actual, color='black', label='ACTUAL')\n",
    "axs[1].plot(wat_simulated, color='red', label='NUM SIMULATION')\n",
    "for i, pred in enumerate(wat_predicted):\n",
    "    linestyle = 'solid' if i == indice_mediana else 'dashed'\n",
    "    alpha = 1.0 if i == indice_mediana else 0.5\n",
    "    label = f'LSTM-NUMS/SEED{i+1}' + ('(S)' if i == indice_mediana else '')\n",
    "    axs[1].plot(pred, color=colors[i], linestyle=linestyle, alpha=alpha, label=label)\n",
    "axs[1].set_title(f'Water Production/Prediction - SPU0{v_UEP}/F01 / from: {GRAPH_TIME}')\n",
    "axs[1].set_xlabel('Days')\n",
    "axs[1].set_ylabel('Water Production')\n",
    "axs[1].legend()\n",
    "    \n",
    "# Ajustar layout e salvar\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'figures/LSTM-NUMS_oil_wat_SPU0{v_UEP}_F01_{GRAPH_YEAR}_{GRAPH_MONTH}.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNN3ZwJQwCWFoijd7P1aFSB",
   "provenance": [
    {
     "file_id": "1DBXC3iVjUN5TgbRIwjYYb9D8uTfLJtFg",
     "timestamp": 1685057630275
    },
    {
     "file_id": "1O_Hs9RMKlVBQ_C33yeVLbMp36EtllVXz",
     "timestamp": 1685057555020
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
